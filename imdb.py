# -*- coding: utf-8 -*-
"""imdb.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sZxvI3VPxxX9RcMT1IKy28yOQkuU6OEd
"""

!pip uninstall -y triton
!pip install triton

!pip install -U transformers accelerate datasets bertviz umap-learn seaborn

# Import libraries
import pandas as pd
from sklearn.model_selection import train_test_split
from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer
import torch
from datasets import Dataset, DatasetDict
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix

! pip install datasets
from datasets import load_dataset
from transformers import AutoTokenizer

imdb_data = load_dataset("imdb")

# Select only 500 samples for training and 500 for testing
train_subset = imdb_data["train"].shuffle(seed=42).select(range(5000))
test_subset = imdb_data["test"].shuffle(seed=42).select(range(5000))

# Initialize the tokenizer
model_ckpt = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_ckpt)

def tokenize(batch):
    return tokenizer(batch['text'], padding=True, truncation=True, max_length=256)

# Apply the tokenization to the selected subsets
train_subset = train_subset.map(tokenize, batched=True)
test_subset = test_subset.map(tokenize, batched=True)

# Prepare the model
model = AutoModelForSequenceClassification.from_pretrained(model_ckpt, num_labels=2)

def compute_metrics(pred):
    labels = pred.label_ids
    preds = pred.predictions.argmax(-1)
    acc = accuracy_score(labels, preds)
    f1 = f1_score(labels, preds, average="weighted")
    return {"accuracy": acc, "f1": f1}

training_args = TrainingArguments(
    output_dir="bert_imdb_sentiment",
    num_train_epochs=2,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    gradient_accumulation_steps=2,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    learning_rate=2e-5,
    weight_decay=0.01,
    logging_dir="logs",
    logging_steps=10,
    save_total_limit=2,
    load_best_model_at_end=True,
    fp16=True
)

# Initialize the Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_subset,
    eval_dataset=test_subset,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics
)

# Train the model
trainer.train()